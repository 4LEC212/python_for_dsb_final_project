{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5542ec",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ec85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_parquet('data/train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6481976",
   "metadata": {},
   "source": [
    "# Step 0: EDA (skrub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "\n",
    "TableReport(df_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7283f",
   "metadata": {},
   "source": [
    "# Step 1: Aggregation per user and per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3baac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def step_1_5_session_features(df_raw):\n",
    "    \"\"\"\n",
    "    Calculates session-based features from raw logs.\n",
    "    A session is defined by a gap of > 30 minutes between events.\n",
    "    \"\"\"\n",
    "    df = df_raw.sort_values(['userId', 'ts']).copy()\n",
    "\n",
    "    # Calculate time difference between consecutive events\n",
    "    df['prev_ts'] = df.groupby('userId')['ts'].shift(1)\n",
    "    df['time_diff'] = (df['ts'] - df['prev_ts']) / 1000 / 60 # in minutes\n",
    "\n",
    "    # New session starts if gap > 30 mins or new user\n",
    "    df['new_session'] = ((df['time_diff'] > 30) | (df['time_diff'].isnull())).astype(int)\n",
    "    df['sessionId_generated'] = df.groupby('userId')['new_session'].cumsum()\n",
    "\n",
    "    # Aggregate per session\n",
    "    session_stats = df.groupby(['userId', 'sessionId_generated']).agg({\n",
    "        'ts': ['min', 'max', 'count'], # Start, End, # Items\n",
    "        'page': lambda x: (x == 'Error').sum() # Errors per session\n",
    "    })\n",
    "    session_stats.columns = ['session_start', 'session_end', 'items_per_session', 'errors_per_session']\n",
    "\n",
    "    # Calculate Session Duration\n",
    "    session_stats['session_duration_min'] = (session_stats['session_end'] - session_stats['session_start']) / 1000 / 60\n",
    "\n",
    "    # Aggregate per User per Day\n",
    "    session_stats['date'] = pd.to_datetime(session_stats['session_start'], unit='ms').dt.floor('D')\n",
    "\n",
    "    daily_session_features = session_stats.groupby(['userId', 'date']).agg({\n",
    "        'session_duration_min': 'mean',\n",
    "        'items_per_session': 'mean',\n",
    "        'errors_per_session': 'mean'\n",
    "    }).rename(columns={\n",
    "        'session_duration_min': 'avg_session_duration_daily',\n",
    "        'items_per_session': 'avg_items_per_session_daily',\n",
    "        'errors_per_session': 'avg_errors_per_session_daily'\n",
    "    }).reset_index()\n",
    "\n",
    "    return daily_session_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def step_1_daily_aggregation(df, df_session_features=None):\n",
    "    \"\"\"\n",
    "    Transforms raw logs into a Daily User Activity df (grouped by userid and day)\n",
    "\n",
    "    Logic:\n",
    "    1. Converts timestamps to daily dates\n",
    "    2. Filter out the target event\n",
    "    3. Feature engineers some specific page events (NextSong, Errors, etc.) into features\n",
    "    4. Aggregates daily metrics like listening time or session count\n",
    "    5. Encodes the 'level' column (paid/free) to binary (paid=1, free=0)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1.\n",
    "    df['date'] = pd.to_datetime(df['ts'], unit='ms').dt.floor('D')\n",
    "\n",
    "    # 2.\n",
    "    df_features = df[df['page'] != 'Cancellation Confirmation']\n",
    "\n",
    "    # 3.\n",
    "    relevant_pages = [\n",
    "        'NextSong', 'Thumbs Up', 'Thumbs Down',\n",
    "        'Error', 'Add Friend', 'Roll Advert', 'Upgrade', 'Downgrade',\n",
    "        'Add to Playlist', 'Settings', 'Help'\n",
    "    ]\n",
    "    df_relevant = df_features[df_features['page'].isin(relevant_pages)]\n",
    "\n",
    "    daily_page_counts = df_relevant.pivot_table(\n",
    "        index=['userId', 'date'],\n",
    "        columns='page',\n",
    "        values='ts',\n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    daily_page_counts.columns = [f'daily_{col.replace(\" \", \"\")}' for col in daily_page_counts.columns]\n",
    "\n",
    "    # 4.\n",
    "    daily_stats = df_features.groupby(['userId', 'date']).agg({\n",
    "        'length': 'sum',              # listening time\n",
    "        'sessionId': 'nunique',       # session count\n",
    "        'level': 'last',              # paid/Free\n",
    "        'registration': 'max',        # registration time\n",
    "        'artist': 'nunique',          # diversity\n",
    "    }).rename(columns={\n",
    "        'length': 'daily_listen_time',\n",
    "        'sessionId': 'daily_sessions',\n",
    "        'level': 'status_level',\n",
    "        'artist': 'daily_unique_artists'})\n",
    "\n",
    "    # merging both df\n",
    "    df_daily = pd.concat([daily_page_counts, daily_stats], axis=1).reset_index()\n",
    "\n",
    "    if df_session_features is not None:\n",
    "        df_daily = df_daily.merge(df_session_features, on=['userId', 'date'], how='left').fillna(0)\n",
    "\n",
    "    # 5.\n",
    "    df_daily['is_paid'] = df_daily['status_level'].apply(lambda x: 1 if x == 'paid' else 0)\n",
    "    df_daily.drop('status_level', axis=1, inplace=True)\n",
    "\n",
    "    return df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29262aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions = step_1_5_session_features(df_train_raw)\n",
    "df_daily_train = step_1_daily_aggregation(df_train_raw, df_session_features=df_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1a4f4",
   "metadata": {},
   "source": [
    "# Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddcfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_feature_engineering(df_daily):\n",
    "    \"\"\"\n",
    "    Feature Engineering focusing on rolling trends and velocity.\n",
    "    \"\"\"\n",
    "    df_daily = df_daily.sort_values(['userId', 'date'])\n",
    "\n",
    "    # 1. Standard Rolling Windows (Shifted to avoid leakage)\n",
    "    features_to_roll = [\n",
    "        'daily_NextSong', 'daily_Error', 'daily_ThumbsDown', 'daily_ThumbsUp',\n",
    "        'daily_RollAdvert', 'daily_listen_time', 'daily_AddFriend', 'daily_Help',\n",
    "        'daily_AddtoPlaylist', 'daily_Downgrade', 'daily_Upgrade', 'daily_sessions',\n",
    "        'daily_unique_artists'\n",
    "    ]\n",
    "\n",
    "    for col in features_to_roll:\n",
    "        grouped = df_daily.groupby('userId')[col]\n",
    "\n",
    "        # Windows: 3d (immediate), 7d (short), 14d (medium), 30d (long)\n",
    "        df_daily[f'{col}_last_3d'] = grouped.transform(lambda x: x.shift(1).rolling(3, min_periods=1).sum())\n",
    "        df_daily[f'{col}_last_7d'] = grouped.transform(lambda x: x.shift(1).rolling(7, min_periods=1).sum())\n",
    "        df_daily[f'{col}_last_14d'] = grouped.transform(lambda x: x.shift(1).rolling(14, min_periods=1).sum())\n",
    "        df_daily[f'{col}_last_30d'] = grouped.transform(lambda x: x.shift(1).rolling(30, min_periods=1).sum())\n",
    "\n",
    "        # 1. Immediate Trend: Is activity dropping in the last 3 days compared to the last 2 weeks?\n",
    "        # A ratio < 1.0 means they are slowing down --> high risk\n",
    "        df_daily[f'{col}_trend_3d_vs_14d'] = df_daily[f'{col}_last_3d'] / (df_daily[f'{col}_last_14d'] / 4.6 + 1)\n",
    "\n",
    "        # 2. Weekly Trend: Is this week worse than last month average?\n",
    "        df_daily[f'{col}_trend_7d_vs_30d'] = df_daily[f'{col}_last_7d'] / (df_daily[f'{col}_last_30d'] / 4.2 + 1)\n",
    "\n",
    "        # 3. Sudden Drop (Delta): Explicitly calculate the drop\n",
    "        df_daily[f'{col}_drop_7d'] = (df_daily[f'{col}_last_7d'] / 7) - (df_daily[f'{col}_last_30d'] / 30)\n",
    "\n",
    "        # Calculate intermediate rolling window for math\n",
    "        grouped = df_daily.groupby('userId')[col]\n",
    "        roll_21d = grouped.transform(lambda x: x.shift(1).rolling(21, min_periods=1).sum())\n",
    "\n",
    "        # Activity from day -30 to day -21 (approx)\n",
    "        slice_1_val = df_daily[f'{col}_last_30d'] - roll_21d\n",
    "\n",
    "        # ratio: current Week vs 4 weeks Ago\n",
    "        df_daily[f'{col}_current_vs_month_ago'] = df_daily[f'{col}_last_7d'] / (slice_1_val + 1)\n",
    "\n",
    "        # rolling variance: only for important features\n",
    "        if col in ['daily_NextSong', 'daily_AddtoPlaylist']:\n",
    "            df_daily[f'{col}_last_7d_var'] = grouped.transform(lambda x: x.shift(1).rolling(7, min_periods=1).var().fillna(0))\n",
    "\n",
    "    # 2. recency & gaps\n",
    "    df_daily['prev_date'] = df_daily.groupby('userId')['date'].shift(1)\n",
    "    df_daily['days_since_last_active'] = (df_daily['date'] - df_daily['prev_date']).dt.days.fillna(500) # 500 for never active\n",
    "    df_daily.drop('prev_date', axis=1, inplace=True)\n",
    "\n",
    "    # variance of inactivity periods (lifetime)\n",
    "    df_daily['time_since_last_action_var'] = df_daily.groupby('userId')['days_since_last_active'].transform(\n",
    "    lambda x: x.shift(1).expanding().var().fillna(0))\n",
    "\n",
    "    # 3. registration lifetime\n",
    "    reg_date = pd.to_datetime(df_daily['registration']).dt.floor('D')\n",
    "    df_daily['days_since_reg'] = (df_daily['date'] - reg_date).dt.days\n",
    "\n",
    "    # 4. Ratios (Intensity)\n",
    "    # Errors per hour (Frustration index)\n",
    "    df_daily['errors_per_hour'] = df_daily['daily_Error_last_7d'] / (df_daily['daily_listen_time_last_7d'] / 3600 + 1)\n",
    "\n",
    "    # Songs per Session (Engagement depth)\n",
    "    df_daily['songs_per_session'] = df_daily['daily_NextSong_last_7d'] / (df_daily['daily_sessions_last_7d'] + 1)\n",
    "\n",
    "    # Clean up\n",
    "    cols_to_drop = ['raw_agent', 'userAgent', 'summary', 'artist']\n",
    "    for c in cols_to_drop:\n",
    "        if c in df_daily.columns:\n",
    "            df_daily.drop(c, axis=1, inplace=True)\n",
    "\n",
    "    return df_daily.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_train = step_2_feature_engineering(df_daily_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7283b",
   "metadata": {},
   "source": [
    "# Step 3: Regroup dataset for a static target (1 row per user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_generate_stacked_dataset(df_features, df_raw_logs, cutoff_dates):\n",
    "    \"\"\"\n",
    "    Generates a stacked dataset by creating multiple snapshots to have more data.\n",
    "    Iterates through cutoff dates, creates a snapshot for each, and stacks them into one large training set.\n",
    "    \"\"\"\n",
    "    stacked_snapshots = []\n",
    "\n",
    "    print(f\"generating stacked dataset across {len(cutoff_dates)} snapshots\")\n",
    "\n",
    "    for cutoff_date in cutoff_dates:\n",
    "        cutoff_ts = pd.Timestamp(cutoff_date)\n",
    "        prediction_window_end = cutoff_ts + pd.Timedelta(days=10)\n",
    "\n",
    "        # 1. filter features: keep only activity BEFORE this specific cutoff\n",
    "        df_history = df_features[df_features['date'] <= cutoff_ts].copy()\n",
    "\n",
    "        # skip if no data available yet\n",
    "        if df_history.empty:\n",
    "            continue\n",
    "\n",
    "        # 2. take the last known state for each user as of this specific cutoff\n",
    "        df_snapshot = df_history.sort_values('date').groupby('userId').tail(1)\n",
    "\n",
    "        # 3. define target: look ahead (10 days after cutoff, just like the final test)\n",
    "        # find users who cancelled in the [cutoff, cutoff+10d] window\n",
    "        churn_events = df_raw_logs[df_raw_logs['page'] == 'Cancellation Confirmation']\n",
    "        churn_events['churn_date'] = pd.to_datetime(churn_events['ts'], unit='ms').dt.floor('D')\n",
    "\n",
    "        churners_in_window = churn_events[\n",
    "            (churn_events['churn_date'] > cutoff_ts) &\n",
    "            (churn_events['churn_date'] <= prediction_window_end)\n",
    "        ]['userId'].unique()\n",
    "\n",
    "        # 4. assign labels (1 = churn in window, 0 = safe)\n",
    "        df_snapshot['target_churn'] = df_snapshot['userId'].isin(churners_in_window).astype(int)\n",
    "\n",
    "        # 5. add date (useful for debugging or splitting later)\n",
    "        df_snapshot['snapshot_date'] = cutoff_ts\n",
    "\n",
    "        stacked_snapshots.append(df_snapshot)\n",
    "        print(f\"--> Snapshot {cutoff_date}: {len(df_snapshot)} users | {df_snapshot['target_churn'].sum()} churners\")\n",
    "\n",
    "    df_stacked = pd.concat(stacked_snapshots, axis=0, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nstacked sataset created. Total rows: {len(df_stacked)}\")\n",
    "    print(f\"Overall class balance: {df_stacked['target_churn'].value_counts().to_dict()}\")\n",
    "\n",
    "    return df_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate a list of dates every 2 days from Oct 7th to Nov 1st\n",
    "# this gives 13 snapshots (approx 3.5 per week)\n",
    "cutoff_dates = pd.date_range(start='2018-10-7', end='2018-11-01', freq='2D').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "print(f\"Cutoff Dates Selected: {cutoff_dates}\")\n",
    "\n",
    "df_stacked_train = step_3_generate_stacked_dataset(df_features_train, df_train_raw, cutoff_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe162a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697df2d",
   "metadata": {},
   "source": [
    "# Step 4: Fine-tuning model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f41abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GroupKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def optimize_with_optuna(df_final, n_trials=50, subsample_ratio=0.4):\n",
    "    \"\"\"\n",
    "    Optimizes hyperparameters using a random sample of the data to save training time.\n",
    "\n",
    "    Args:\n",
    "        df_final: the full stacked dataframe\n",
    "        n_trials: number of optuna trials\n",
    "        subsample_ratio: fraction of data to use for optimization (0.4 = 40%).\n",
    "    \"\"\"\n",
    "    # 1. prepare data & downsample for speed\n",
    "    # we maintain the class ratio while downsampling\n",
    "\n",
    "    if subsample_ratio < 1.0:\n",
    "        print(f\"--> downsampling dataset to {subsample_ratio*100}% for fast optimization\")\n",
    "        # Stratified sample to keep churn rate consistent\n",
    "        df_optim_sample = df_final.groupby('target_churn', group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=subsample_ratio, random_state=5)\n",
    "        )\n",
    "    else:\n",
    "        df_optim_sample = df_final.copy()\n",
    "\n",
    "    print(f\"optimization data shape: {df_optim_sample.shape}\")\n",
    "\n",
    "    groups = df_optim_sample['userId']\n",
    "\n",
    "    # drop metadata for optuna optimization\n",
    "    cols_to_drop = ['userId', 'date', 'registration', 'target_churn', 'snapshot_date']\n",
    "    existing_drop = [c for c in cols_to_drop if c in df_optim_sample.columns]\n",
    "\n",
    "    X = df_optim_sample.drop(columns=existing_drop)\n",
    "    y = df_optim_sample['target_churn']\n",
    "\n",
    "    # calculate scale pos weight based on the sample\n",
    "    ratio = (y == 0).sum() / (y == 1).sum()\n",
    "    print(f\"sample class imbalance ratio: {ratio:.2f}\")\n",
    "\n",
    "    # 2. feature selection (from a simplified model)\n",
    "    print(\"--> Running Feature Selection\")\n",
    "    selector = xgb.XGBClassifier(\n",
    "        learning_rate=0.03,\n",
    "        max_depth=4,\n",
    "        random_state=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    selection = SelectFromModel(selector, threshold='0.6*mean', prefit=True)\n",
    "    X_selected = pd.DataFrame(selection.transform(X), columns=X.columns[selection.get_support()])\n",
    "\n",
    "    final_features = X_selected.columns.tolist()\n",
    "    print(f\"selected {len(final_features)} features\")\n",
    "\n",
    "    # 3. Define Objective\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 300, 600),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "            'min_child_weight': trial.suggest_float('min_child_weight', 1, 5),\n",
    "            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 5, ratio),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 5.0, 15.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 2.0),\n",
    "\n",
    "            # Speed optimizations\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'tree_method': 'hist',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 5\n",
    "        }\n",
    "\n",
    "        gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "\n",
    "        scores = cross_val_score(model, X_selected, y, cv=gkf, groups=groups, scoring='roc_auc', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    # 4. Run Optimization\n",
    "    print(f\"\\n--> starting optuna ({n_trials} trials)\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "    # enqueue known good params to speed up convergence\n",
    "    study.enqueue_trial({\n",
    "        'max_depth': 3,\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.7\n",
    "    })\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(f\"--> Best AUC on Sample: {study.best_value:.4f}\")\n",
    "\n",
    "    best_params = study.best_params.copy()\n",
    "    # add back the standard static params\n",
    "    best_params.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'tree_method': 'hist',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 5\n",
    "    })\n",
    "\n",
    "    return best_params, final_features\n",
    "\n",
    "best_params_dict, final_features = optimize_with_optuna(df_stacked_train, n_trials=50, subsample_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c226a31",
   "metadata": {},
   "source": [
    "# Step 5: 5-fold grouped prediction + generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c946e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def train_stacked_ensemble(df_final, params, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains a robust ensemble and returns OOF predictions for plotting.\n",
    "    \"\"\"\n",
    "    cols_to_drop = ['userId', 'date', 'registration', 'target_churn', 'snapshot_date']\n",
    "    existing_drop = [c for c in cols_to_drop if c in df_final.columns]\n",
    "\n",
    "    X = df_final.drop(columns=existing_drop)\n",
    "    y = df_final['target_churn']\n",
    "    groups = df_final['userId']\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    models = []\n",
    "    oof_preds = np.zeros(len(X))\n",
    "\n",
    "    print(f\"starting {n_splits}-fold group training on stacked data\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "        val_probs = model.predict_proba(X_val)[:, 1]\n",
    "        oof_preds[val_idx] = val_probs\n",
    "        models.append(model)\n",
    "\n",
    "        score = roc_auc_score(y_val, val_probs)\n",
    "        print(f\"fold {fold+1} AUC: {score:.4f}\")\n",
    "\n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0\n",
    "    for thresh in np.arange(0.2, 0.6, 0.01):\n",
    "        preds = (oof_preds >= thresh).astype(int)\n",
    "        score = f1_score(y, preds)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_thresh = thresh\n",
    "\n",
    "    print(f\"\\noverall OOF AUC: {roc_auc_score(y, oof_preds):.4f}\")\n",
    "    print(f\"OOF 'Natural' Threshold: {best_thresh:.3f} (F1: {best_f1:.4f})\")\n",
    "\n",
    "    # we return 'y' and 'oof_preds' so we can plot curves later\n",
    "    return models, feature_names, y, oof_preds\n",
    "\n",
    "def generate_final_submission_with_target_rate(models, test_path, feature_cols, target_churn_rate=0.4):\n",
    "    \"\"\"\n",
    "    Generates submission and dynamically finds the threshold\n",
    "    to match the exact 'target_churn_rate' (e.g. 0.4)\n",
    "    \"\"\"\n",
    "    df_test_raw = pd.read_parquet(test_path)\n",
    "\n",
    "    # reuse pipeline\n",
    "    df_sessions = step_1_5_session_features(df_test_raw)\n",
    "    df_daily_test = step_1_daily_aggregation(df_test_raw, df_session_features=df_sessions)\n",
    "    df_features_test = step_2_feature_engineering(df_daily_test)\n",
    "\n",
    "    # snapshot last day\n",
    "    df_test_last = df_features_test.sort_values('date').groupby('userId').tail(1)\n",
    "\n",
    "    # align columns\n",
    "    X_test = df_test_last.copy()\n",
    "    for col in feature_cols:\n",
    "        if col not in X_test.columns:\n",
    "            X_test[col] = 0\n",
    "    X_test = X_test[feature_cols]\n",
    "\n",
    "    # predict\n",
    "    print(f\"predicting with {len(models)}-model ensemble\")\n",
    "    avg_probs = np.zeros(len(X_test))\n",
    "    for model in models:\n",
    "        avg_probs += model.predict_proba(X_test)[:, 1]\n",
    "    avg_probs = avg_probs / len(models)\n",
    "\n",
    "    # CALCULATE THRESHOLD FOR EXACT RATE\n",
    "    # to get top 40%, we need the (100% - 40%) = 60th percentile\n",
    "    quantile = 1 - target_churn_rate\n",
    "    forced_threshold = np.quantile(avg_probs, quantile)\n",
    "\n",
    "    print(f\"\\ntarget churn rate: {target_churn_rate:.2%}\")\n",
    "    print(f\"--> calculated threshold: {forced_threshold:.5f}\")\n",
    "\n",
    "    # create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': df_test_last['userId'],\n",
    "        'target_prob': avg_probs,\n",
    "        'target_binary': (avg_probs >= forced_threshold).astype(int)\n",
    "    })\n",
    "\n",
    "    sub_name_bin = f'submissions/submission_final_binary.csv'\n",
    "    sub_name_prob = f'submissions/submission_final_prob.csv'\n",
    "\n",
    "    submission[['id', 'target_binary']].rename(columns={'target_binary': 'target'}).to_csv(sub_name_bin, index=False)\n",
    "    submission[['id', 'target_prob']].rename(columns={'target_prob': 'target'}).to_csv(sub_name_prob, index=False)\n",
    "\n",
    "    print(f\"saved: {sub_name_bin}\")\n",
    "    print(f\"final predicted churn rate: {submission['target_binary'].mean():.2%}\")\n",
    "\n",
    "    return submission\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "def plot_performance_curves(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    plots ROC and precision-recall curves side by side.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # 1. ROC curve\n",
    "    RocCurveDisplay.from_predictions(y_true, y_scores, ax=ax1, name=\"our XGBoost model\")\n",
    "    ax1.plot([0, 1], [0, 1], \"k--\", label=\"random chance\")\n",
    "    ax1.set_title(\"ROC curve\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 2. precision-recall curve\n",
    "    PrecisionRecallDisplay.from_predictions(y_true, y_scores, ax=ax2, name=\"our XGBoost model\")\n",
    "    ax2.set_title(\"precision-recall curve\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_cumulative_gain(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    plots the cmulative gain curve (\"banana curve\").\n",
    "    shows what % of total churners we catch by targeting the top X% of users\n",
    "    (in terms of potential churners)\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame({'true': y_true, 'score': y_scores})\n",
    "    data = data.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # calculate cumulative stats\n",
    "    data['cumulative_churners'] = data['true'].cumsum()\n",
    "    total_churners = data['true'].sum()\n",
    "\n",
    "    # calculate percentages (x and y axes)\n",
    "    data['percent_users_contacted'] = (data.index + 1) / len(data) * 100\n",
    "    data['percent_churners_found'] = data['cumulative_churners'] / total_churners * 100\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(data['percent_users_contacted'], data['percent_churners_found'],\n",
    "             label='our XGBoost model', color='tab:blue', linewidth=2)\n",
    "\n",
    "    # baseline (random guessing)\n",
    "    plt.plot([0, 100], [0, 100], 'k--', label='Random Guessing', alpha=0.5)\n",
    "\n",
    "    # formatting\n",
    "    plt.title('Cumulative Gains Curve (the \"banana\" chart)')\n",
    "    plt.xlabel('% of Users Contacted')\n",
    "    plt.ylabel('% of Total Churners Found')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # highlight the top 40% point (the target churn rate)\n",
    "    top_40_idx = int(len(data) * 0.40)\n",
    "    recall_at_40 = data.loc[top_40_idx, 'percent_churners_found']\n",
    "\n",
    "    plt.plot(40, recall_at_40, 'ro')\n",
    "    plt.annotate(f\"top 40% -> Captures {recall_at_40:.1f}% of churners\",\n",
    "                 xy=(40, recall_at_40), xytext=(45, recall_at_40-15),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTION\n",
    "\n",
    "# 1. get params from step 4\n",
    "final_params = best_params_dict.copy()\n",
    "final_params['random_state'] = 5\n",
    "final_params['n_jobs'] = -1\n",
    "\n",
    "# 2. train models & get OOF predictions\n",
    "models, feature_names, y_true, oof_preds = train_stacked_ensemble(df_stacked_train, final_params)\n",
    "\n",
    "# 3. plot curves\n",
    "print(\"\\ngenerating performance plots:\")\n",
    "plot_performance_curves(y_true, oof_preds)\n",
    "plot_cumulative_gain(y_true, oof_preds)\n",
    "\n",
    "# 4. generate Submission with exact 40% churn\n",
    "sub = generate_final_submission_with_target_rate(\n",
    "    models,\n",
    "    test_path='data/test.parquet',\n",
    "    feature_cols=feature_names,\n",
    "    target_churn_rate=0.4\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
