# ðŸŽµ Music Streaming Churn Prediction

**Authors:** Alec & [@silabou](https://github.com/silabou)

**Context:** Python for Data Science class @ Ã‰cole Polytechnique

## ðŸ“– Project Overview

This repository contains a complete Machine Learning pipeline designed to predict user churn for a music streaming service. Using raw user activity logs (listening history, page visits, errors, etc.), we engineered time-series features to classify whether a user is likely to cancel their subscription.

The goal was to build a robust model capable of handling class imbalance and preventing overfitting on the leaderboard metric.

## ðŸš€ Key Features

* **Automated ETL:** Aggregates raw logs into daily user snapshots.

* **Time-Series Feature Engineering:** Generates rolling window statistics (7-day, 30-day), trends, and lifetime accumulations.

* **Robust Validation:** Uses Stratified K-Fold Cross-Validation to ensure model stability.

* **Dynamic Thresholding:** Tunes the classification threshold based on Out-Of-Fold (OOF) predictions to maximize the F1-Score.

## ðŸ§  Iterative Modeling Strategy

We approached this problem iteratively to improve performance and stability:

1. **Baseline Model (XGBoost):**

   * Initial feature engineering (daily aggregations).

   * Trained on a simple 80/20 Train/Validation split.

   * *Result:* Good initial AUC, but high variance.

2. **Feature Selection & Cleanup:**

   * Implemented `SelectFromModel` to aggressively prune noisy features (dropping those below 0.5x mean importance).

   * Reduced dimensionality while maintaining predictive power.

3. **Hyperparameter Optimization:**

   * Performed `RandomizedSearchCV` to narrow down the search space.

   * Refined with `GridSearchCV` to lock in optimal regularization (L1/L2), learning rate, and tree depth.

4. **Final Approach: 5-Fold Ensemble (Current State):**

   * **Problem:** We observed overfitting to the validation set when tuning thresholds.

   * **Solution:** Switched to **5-Fold Cross-Validation**. Instead of relying on one model, we train 5 separate models on different data splits and average their predictions.

   * **Outcome:** A highly stable score that generalizes better to unseen test data.

## ðŸ“Š Results

The model outputs two files:

1. `submission_kfold_avg_prob.csv`: Raw probabilities (useful for ROC-AUC analysis).

2. `submission_kfold_avg_binary.csv`: Final churn predictions (0 or 1) based on the optimal OOF threshold.

*This project is part of the academic coursework at Ã‰cole Polytechnique.*